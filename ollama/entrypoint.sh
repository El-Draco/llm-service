#!/bin/sh

# Pull the model
ollama pull llama3

# Start the Ollama server
ollama serve